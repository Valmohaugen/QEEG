{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bae13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ qnn_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QNNLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m288\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ qnn_layer_2 (\u001b[38;5;33mQNNLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m4\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">556</span> (2.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m556\u001b[0m (2.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">556</span> (2.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m556\u001b[0m (2.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting standard training...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fenlei\\myenv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 16s/step - accuracy: 0.5225 - auc: 0.5157 - loss: 0.6928 - val_accuracy: 0.6250 - val_auc: 0.6445 - val_loss: 0.6740\n",
      "Epoch 2/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 13s/step - accuracy: 0.5882 - auc: 0.6164 - loss: 0.6849 - val_accuracy: 0.6667 - val_auc: 0.6523 - val_loss: 0.6728\n",
      "Epoch 3/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 14s/step - accuracy: 0.5398 - auc: 0.5515 - loss: 0.7131 - val_accuracy: 0.6667 - val_auc: 0.6680 - val_loss: 0.6703\n",
      "Epoch 4/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 30s/step - accuracy: 0.4920 - auc: 0.4873 - loss: 0.7050 - val_accuracy: 0.6667 - val_auc: 0.6758 - val_loss: 0.6697\n",
      "Epoch 5/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 25s/step - accuracy: 0.5541 - auc: 0.6039 - loss: 0.6889 - val_accuracy: 0.6667 - val_auc: 0.6914 - val_loss: 0.6679\n",
      "Epoch 6/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 9s/step - accuracy: 0.5878 - auc: 0.6180 - loss: 0.6777 - val_accuracy: 0.6667 - val_auc: 0.7070 - val_loss: 0.6650\n",
      "Epoch 7/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 10s/step - accuracy: 0.5072 - auc: 0.5299 - loss: 0.7029 - val_accuracy: 0.7083 - val_auc: 0.7188 - val_loss: 0.6638\n",
      "Epoch 8/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 11s/step - accuracy: 0.5658 - auc: 0.6654 - loss: 0.6656 - val_accuracy: 0.6667 - val_auc: 0.7227 - val_loss: 0.6622\n",
      "Epoch 9/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 25s/step - accuracy: 0.5818 - auc: 0.6243 - loss: 0.6709 - val_accuracy: 0.6250 - val_auc: 0.7070 - val_loss: 0.6616\n",
      "Epoch 10/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 14s/step - accuracy: 0.5520 - auc: 0.5990 - loss: 0.6813 - val_accuracy: 0.6250 - val_auc: 0.7031 - val_loss: 0.6606\n",
      "Epoch 11/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 29s/step - accuracy: 0.5827 - auc: 0.5839 - loss: 0.7049 - val_accuracy: 0.5833 - val_auc: 0.7070 - val_loss: 0.6599\n",
      "Epoch 12/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 10s/step - accuracy: 0.6491 - auc: 0.7206 - loss: 0.6179 - val_accuracy: 0.5833 - val_auc: 0.7109 - val_loss: 0.6575\n",
      "Epoch 13/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 18s/step - accuracy: 0.5799 - auc: 0.6238 - loss: 0.6820 - val_accuracy: 0.6250 - val_auc: 0.7227 - val_loss: 0.6568\n",
      "Epoch 14/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 10s/step - accuracy: 0.6396 - auc: 0.6485 - loss: 0.6635 - val_accuracy: 0.6250 - val_auc: 0.7383 - val_loss: 0.6573\n",
      "Epoch 15/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 10s/step - accuracy: 0.6586 - auc: 0.6770 - loss: 0.6516 - val_accuracy: 0.6250 - val_auc: 0.7266 - val_loss: 0.6577\n",
      "Epoch 16/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 16s/step - accuracy: 0.6152 - auc: 0.6535 - loss: 0.6602 - val_accuracy: 0.6250 - val_auc: 0.7266 - val_loss: 0.6581\n",
      "Epoch 17/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 9s/step - accuracy: 0.5727 - auc: 0.5531 - loss: 0.6976 - val_accuracy: 0.6250 - val_auc: 0.7305 - val_loss: 0.6571\n",
      "Epoch 18/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 9s/step - accuracy: 0.6411 - auc: 0.6672 - loss: 0.6647 - val_accuracy: 0.6250 - val_auc: 0.7383 - val_loss: 0.6561\n",
      "Epoch 19/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 9s/step - accuracy: 0.6079 - auc: 0.6468 - loss: 0.6506 - val_accuracy: 0.6250 - val_auc: 0.7383 - val_loss: 0.6554\n",
      "Epoch 20/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 9s/step - accuracy: 0.5453 - auc: 0.6425 - loss: 0.6609 - val_accuracy: 0.6250 - val_auc: 0.7500 - val_loss: 0.6545\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.7133 - auc: 0.7142 - loss: 0.6360\n",
      "Test loss: 0.6442\n",
      "Test accuracy: 0.6949\n",
      "Test AUC: 0.7066\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "Test AUC (from ROC): 0.708\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, pennylane as qml, tensorflow as tf\n",
    "from tensorflow.keras.layers import Lambda, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from pennylane.templates import AmplitudeEmbedding, StronglyEntanglingLayers\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "#1. Data Collection and Preparation\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Set TensorFlow to use float32 and eager execution for better quantum debugging\n",
    "tf.keras.backend.set_floatx(\"float32\")\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Hyperparameters for model setup\n",
    "CSV_PATH   = \"C:\\\\Users\\\\fenlei\\\\Downloads\\\\machinelearning_data_EEG.csv\"\n",
    "N_QUBITS   = 3                # 2**4 = 16 amplitudes\n",
    "AMP_DIM    = 2 ** N_QUBITS\n",
    "N_LAYERS   = 3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS     = 20\n",
    "VAL_SPLIT  = 0.1\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv(CSV_PATH).dropna(axis=1, how=\"all\")\n",
    "target = df.select_dtypes(include=\"object\").columns[0]\n",
    "y = LabelEncoder().fit_transform(df[target]).astype(\"float32\")\n",
    "\n",
    "# Dimensionality reduction to match quantum circuit input\n",
    "X = df.drop(columns=[target]).select_dtypes(\"number\")\n",
    "X = SimpleImputer(strategy=\"mean\").fit_transform(X)\n",
    "X = StandardScaler().fit_transform(X).astype(\"float32\")\n",
    "X = PCA(n_components=AMP_DIM).fit_transform(X).astype(\"float32\")   # 16 feats\n",
    "\n",
    "# Split data into train/test sets\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_tr, y_tr, test_size=VAL_SPLIT, stratify=y_tr, random_state=42\n",
    ")\n",
    "\n",
    "# Compute class weights for imbalanced datasets\n",
    "class_weight = dict(enumerate(\n",
    "    compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "))\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "#2. Quantum Layers\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define a custom quantum layer\n",
    "class QNNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_qubits, n_layers):\n",
    "        super().__init__(dtype=tf.float32)\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Initialize weights\n",
    "        init_weights = tf.random.uniform(\n",
    "            shape=(n_layers, n_qubits, 3),\n",
    "            minval=0,\n",
    "            maxval=2*np.pi,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        self.qweights = tf.Variable(init_weights, dtype=tf.float32, trainable=True)\n",
    "        \n",
    "        # Define quantum circuit\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "        \n",
    "        @qml.qnode(dev, interface=\"tf\", diff_method=\"parameter-shift\")\n",
    "        def circuit(inputs, weights):\n",
    "            # Let the AmplitudeEmbedding handle normalization automatically.\n",
    "            AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True)\n",
    "            StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "        \n",
    "        self.circuit = circuit\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # We'll process one sample at a time\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        output_tensor = tf.TensorArray(tf.float32, size=batch_size)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Process single input\n",
    "            single_input = tf.cast(inputs[i], tf.float32)\n",
    "            # Manual normalization before quantum circuit\n",
    "            single_result = self.circuit(single_input, self.qweights)\n",
    "            single_result = tf.cast(single_result, tf.float32)\n",
    "            output_tensor = output_tensor.write(i, single_result)\n",
    "        \n",
    "        return output_tensor.stack()\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.n_qubits)\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "#3. Building a Hybrid Classical-Quantum Model\n",
    "#   a. Classical layers preprocess EEG features.\n",
    "#   b. Quantum layer (QNNLayer) transforms features with quantum operations.\n",
    "#   c. Final classical layer makes the binary prediction.\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Hybrid classical-quantum model\n",
    "inp = tf.keras.Input(shape=(AMP_DIM,), dtype=tf.float32)\n",
    "x   = tf.keras.layers.Dense(32, activation=\"relu\", dtype=tf.float32)(inp)\n",
    "x   = Dropout(0.2)(x)\n",
    "x   = tf.keras.layers.Dense(AMP_DIM, dtype=tf.float32)(x)\n",
    "\n",
    "# Use the simplified QNN layer\n",
    "x   = QNNLayer(N_QUBITS, N_LAYERS)(x)\n",
    "out = tf.keras.layers.Dense(1, activation=\"sigmoid\", dtype=tf.float32)(x)\n",
    "\n",
    "model = tf.keras.Model(inp, out)\n",
    "\n",
    "# Use a standard optimizer with a small learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4, epsilon=1e-8)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")])\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "#4. Training Models\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Convert all tensors explicitly to float32 to be ready for training\n",
    "x_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "x_val_tensor = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "# Print model summary before training\n",
    "model.summary()\n",
    "\n",
    "# Trying implicit normalization in amplitude embedding\n",
    "try:\n",
    "    print(\"\\nAttempting standard training...\")\n",
    "    model.fit(\n",
    "        x_train_tensor, y_train_tensor,\n",
    "        validation_data=(x_val_tensor, y_val_tensor),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_metrics = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\n",
    "    print(f\"Test loss: {test_metrics[0]:.4f}\")\n",
    "    print(f\"Test accuracy: {test_metrics[1]:.4f}\")\n",
    "    print(f\"Test AUC: {test_metrics[2]:.4f}\")\n",
    "    \n",
    "    # Get ROC curve\n",
    "    y_prob = model.predict(X_test, batch_size=BATCH_SIZE).flatten()\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    test_auc = auc(fpr, tpr)\n",
    "    print(f\"Test AUC (from ROC): {test_auc:.3f}\")\n",
    "# Manual normalization\n",
    "except Exception as e:\n",
    "    print(f\"\\nStandard training failed with error: {str(e)}\")\n",
    "    print(\"\\nFalling back to custom training...\")\n",
    "    \n",
    "    # If standard training fails, try a custom training loop\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4, epsilon=1e-8)\n",
    "    \n",
    "    # Custom training loop with manual batch processing\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        # Shuffle training data\n",
    "        indices = tf.range(start=0, limit=tf.shape(x_train_tensor)[0], dtype=tf.int32)\n",
    "        shuffled_indices = tf.random.shuffle(indices)\n",
    "        x_shuffled = tf.gather(x_train_tensor, shuffled_indices)\n",
    "        y_shuffled = tf.gather(y_train_tensor, shuffled_indices)\n",
    "        \n",
    "        # Process in batches\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for i in range(0, len(x_shuffled), BATCH_SIZE):\n",
    "            end_idx = min(i + BATCH_SIZE, len(x_shuffled))\n",
    "            x_batch = x_shuffled[i:end_idx]\n",
    "            y_batch = y_shuffled[i:end_idx]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                y_pred = model(x_batch, training=True)\n",
    "                \n",
    "                # Calculate loss with class weights\n",
    "                loss_values = tf.keras.losses.binary_crossentropy(y_batch, y_pred)\n",
    "                sample_weights = tf.ones_like(y_batch)\n",
    "                \n",
    "                for cls, weight in class_weight.items():\n",
    "                    cls_mask = tf.cast(tf.equal(y_batch, cls), tf.float32)\n",
    "                    sample_weights = sample_weights * cls_mask * weight + (1 - cls_mask) * sample_weights\n",
    "                \n",
    "                weighted_loss = loss_values * sample_weights\n",
    "                batch_loss = tf.reduce_mean(weighted_loss)\n",
    "            \n",
    "            # Compute and apply gradients\n",
    "            grads = tape.gradient(batch_loss, model.trainable_variables)\n",
    "            # Clip gradients to prevent explosion\n",
    "            grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "            # Force to float32\n",
    "            grads = [tf.cast(g, tf.float32) if g is not None else None for g in grads]\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            num_batches += 1\n",
    "            \n",
    "        # Calculate average loss for epoch\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        val_pred = model.predict(x_val_tensor, verbose=0)\n",
    "        val_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_val_tensor, val_pred))\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.round(val_pred), y_val_tensor), tf.float32))\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "#5. Evaluate Results\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_pred = model.predict(X_test, verbose=0)\n",
    "    test_acc = tf.reduce_mean(tf.cast(tf.equal(tf.round(test_pred), y_test), tf.float32))\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # ROC curve\n",
    "    y_prob = test_pred.flatten()\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    test_auc = auc(fpr, tpr)\n",
    "    print(f\"Test AUC: {test_auc:.3f}\")\n",
    "    # Save the Model\n",
    "    model.save(\"QML1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c57fe6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26,  4],\n",
       "       [14, 15]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix([int(y>0.5) for y in y_prob], y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
